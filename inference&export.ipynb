{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on images using PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeldar/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from utils.feature_extractor import featureExtractor\n",
    "from utils.data_loader import TestDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from time import time\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(trained_model, dataset_dir):\n",
    "    img_list = os.listdir(dataset_dir)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    names = []\n",
    "    scores = []\n",
    "    labels = []\n",
    "    # st = time()\n",
    "    for ind, image_name in enumerate(img_list):\n",
    "\n",
    "        # try:\n",
    "        print(\"Blurry Image Prediction: %d / %d images processed..\" % (ind, len(img_list)))\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(os.path.join(dataset_dir, image_name), 0)\n",
    "        prediction, score = is_image_blurry(trained_model, img, threshold=0.5)\n",
    "        # print(prediction, score)\n",
    "        label = int(image_name.split('.')[0][-1])\n",
    "        if (prediction):\n",
    "            if (label):\n",
    "                fp += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if (label):\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        names.append(image_name)\n",
    "        scores.append(score)\n",
    "        labels.append(label)\n",
    "        # except:\n",
    "        #     continue\n",
    "\n",
    "    # df = pd.DataFrame({\"Name\": names, \"Score\": scores, \"Labels\": labels})\n",
    "    # e = time()\n",
    "    # print(\"SPENT Time\", e-st)\n",
    "    print(tp, fp, tn, fn)\n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    acc = (tp+tn)/(tp+tn+fn+fp)\n",
    "    print(\"Precision: \", p, \"Recall: \", r, \"F1-score: \", f1, \"Accuracy: \", acc)\n",
    "    # return df\n",
    "\n",
    "\n",
    "def is_image_blurry(trained_model, img, threshold=0.5):\n",
    "    feature_extractor = featureExtractor()\n",
    "    accumulator = []\n",
    "\n",
    "    # Resize the image by the downsampling factor\n",
    "    feature_extractor.resize_image(img, np.shape(img)[0], np.shape(img)[1])\n",
    "\n",
    "    # compute the image ROI using local entropy filter\n",
    "    feature_extractor.compute_roi()\n",
    "\n",
    "    # extract the blur features using DCT transform coefficients\n",
    "    extracted_features = feature_extractor.extract_feature()\n",
    "    extracted_features = np.array(extracted_features)\n",
    "\n",
    "    if(len(extracted_features) == 0):\n",
    "        return True\n",
    "    test_data_loader = DataLoader(TestDataset(extracted_features), batch_size=1, shuffle=False)\n",
    "    # ort_sess = ort.InferenceSession('blur.onnx')\n",
    "    for batch_num, input_data in enumerate(test_data_loader):\n",
    "        x = input_data\n",
    "        x = x.to(device).float()\n",
    "        output = trained_model(x)\n",
    "        # output = ort_sess.run(None, {'input': x.numpy()})\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "        accumulator.append(predicted_label.item())\n",
    "    \n",
    "    prediction= np.mean(accumulator) < threshold\n",
    "    return prediction, np.mean(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load('./trained_model/trained_model-Kaggle_dataset1')\n",
    "trained_model = trained_model['model_state'].to(device)\n",
    "# trained_model = onnx.load(\"blur.onnx\")\n",
    "# dataset_dir = '/home/yeldar/Documents/bluriness/data/test2'\n",
    "# run_testing(trained_model, dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inference on single image using ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(session, img): \n",
    "    #Run inference using ONNX model\n",
    "    #Parameters:\n",
    "    #    session (onnxruntime.InferenceSession): inference session based on provided onnx model \n",
    "    #    img (str): path to image \n",
    "\n",
    "    #Returns:\n",
    "    #    prediction (bool): True if image blurred, False otherwise\n",
    "    #    score (float): prediction score returned by model \n",
    "\n",
    "    feature_extractor = featureExtractor()\n",
    "    accumulator = []\n",
    "    threshold=0.5\n",
    "    feature_extractor.resize_image(img, np.shape(img)[0], np.shape(img)[1])\n",
    "\n",
    "    # compute the image ROI using local entropy filter\n",
    "    feature_extractor.compute_roi()\n",
    "\n",
    "    # # extract the blur features using DCT transform coefficients\n",
    "    extracted_features = feature_extractor.extract_feature()\n",
    "    extracted_features = np.array(extracted_features)\n",
    "\n",
    "    if(len(extracted_features) == 0):\n",
    "        print(\"Error processing features\")\n",
    "\n",
    "    test_data_loader = DataLoader(TestDataset(extracted_features), batch_size=1, shuffle=False)\n",
    "\n",
    "    for batch_num, input_data in enumerate(test_data_loader):\n",
    "        x = input_data\n",
    "        x = x.float()\n",
    "        x = np.resize(x,(1,x.shape[1]))\n",
    "        output = session.run(['output'], {'input': x})\n",
    "        output = torch.tensor(output[0])\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "        accumulator.append(predicted_label.item())\n",
    "    \n",
    "    score = np.mean(accumulator)\n",
    "    prediction_label = score < threshold\n",
    "    return prediction_label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0.7723577235772358\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"0_1.jpg\", 0)\n",
    "ort_sess = ort.InferenceSession('blur.onnx', None)\n",
    "pred, score = predict(ort_sess, img)\n",
    "print(pred, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Convert to ONNX \n",
    "def Convert_ONNX(model): \n",
    "\n",
    "    # set the model to inference mode \n",
    "    model.eval() \n",
    "\n",
    "    dummy_input = torch.randn(1, 528, requires_grad=False).to(device)\n",
    "    onnx_name = \"blur.onnx\"\n",
    "    torch.onnx.export(model, dummy_input, onnx_name, verbose=True,\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']) # the model's output names\n",
    "    \n",
    "    # export_output.save(onnx_name)\n",
    "    print(\" \") \n",
    "    print('Model has been converted to ONNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=528, out_features=64, bias=True)\n",
      "Exported graph: graph(%input : Float(1, 528, strides=[528, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(64, 528, strides=[528, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(32, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(2, 32, strides=[32, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/fc1/Gemm_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%input, %fc1.weight, %fc1.bias), scope: utils.MLP.MLP::/torch.nn.modules.linear.Linear::fc1 # /home/yeldar/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/Relu\"](%/fc1/Gemm_output_0), scope: utils.MLP.MLP:: # /home/yeldar/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 32, strides=[32, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Relu_output_0, %fc2.weight, %fc2.bias), scope: utils.MLP.MLP::/torch.nn.modules.linear.Linear::fc2 # /home/yeldar/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_1_output_0 : Float(1, 32, strides=[32, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/Relu_1\"](%/fc2/Gemm_output_0), scope: utils.MLP.MLP:: # /home/yeldar/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %output : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Relu_1_output_0, %fc3.weight, %fc3.bias), scope: utils.MLP.MLP::/torch.nn.modules.linear.Linear::fc3 # /home/yeldar/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%output)\n",
      "\n",
      " \n",
      "Model has been converted to ONNX\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Convert_ONNX(trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
